<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Projects | HanCui</title>
    <link>https://cccccent.github.com/project/</link>
      <atom:link href="https://cccccent.github.com/project/index.xml" rel="self" type="application/rss+xml" />
    <description>Projects</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Wed, 27 Apr 2016 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://cccccent.github.com/media/icon_hu5ff4e746f35b000630ba3832250142a8_14680_512x512_fill_lanczos_center_3.png</url>
      <title>Projects</title>
      <link>https://cccccent.github.com/project/</link>
    </image>
    
    <item>
      <title>A token-level explanation extraction framework.</title>
      <link>https://cccccent.github.com/project/example/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      <guid>https://cccccent.github.com/project/example/</guid>
      <description>&lt;p&gt;In the task of token-level explanation extraction, the model is tasked with generating an answer based on the given context and question, while simultaneously extracting evidence supporting the answer at the token level.&lt;/p&gt;
&lt;p&gt;In our proposed approach, we present a framework for explanation extraction that incorporates multi-layer filtering. Firstly, a coarse retrieval of the context is conducted using the generated answer to estimate the approximate location of the answer. After obtaining the content from the coarse retrieval, we introduce perturbations to each token, carefully evaluating the effects of these changes on the overall sentence semantics and the subsequent impact on answer confidence. This rigorous analysis ensures that the perturbed tokens serve as reliable evidence for supporting the answer.&lt;/p&gt;
&lt;p&gt;In this project, I served as the principal investigator, responsible for the method design and primary experiments. Our team achieved 9th place (9/141) in the CCF-BDCI Interpretable Reading Comprehension Evaluation based on the Wenxin NLP large-scale model.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>External Project</title>
      <link>https://cccccent.github.com/project/external-project/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      <guid>https://cccccent.github.com/project/external-project/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
